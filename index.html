<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Right AI Robot</title>
    <!-- Inter font from Google Fonts -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap">
    <!-- Tailwind CSS CDN for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- React and ReactDOM from CDN -->
    <script src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <!-- Babel for JSX compilation -->
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        /* The fade-out animation has been removed to keep the caption persistent. */
    </style>
</head>
<body class="bg-gray-950 text-white">
    <!-- The root element where the React app will be rendered. -->
    <div id="root"></div>

    <script type="text/babel">
    const { useState, useEffect, useRef } = React;

    // Main application component for the AI robot.
    const App = () => {
        // State to manage the conversation history.
        const [chatHistory, setChatHistory] = useState([]);
        // State for the user's current question.
        const [currentQuestion, setCurrentQuestion] = useState('');
        // State for the AI's current answer.
        const [currentAnswer, setCurrentAnswer] = useState('');
        // State to control microphone listening.
        const [isListening, setIsListening] = useState(false);
        // State to control the AI speaking animation.
        const [isSpeaking, setIsSpeaking] = useState(false);
        // State to show a general loading indicator.
        const [isLoading, setIsLoading] = useState(false);
        // State to show a loading indicator for text generation.
        const [isGeneratingText, setIsGeneratingText] = useState(false);
        // State to show a loading indicator for audio generation.
        const [isGeneratingAudio, setIsGeneratingAudio] = useState(false);
        // State for chat history visibility.
        const [showChatHistory, setShowChatHistory] = useState(false);
        // State for fun fact generation mode.
        const [isGeneratingFact, setIsGeneratingFact] = useState(false);
        // State to select the AI's voice.
        const [voiceName, setVoiceName] = useState("Kore");
        // Ref to manage the file input element for image uploads.
        const fileInputRef = useRef(null);
        // Ref to the chat history container for auto-scrolling.
        const chatHistoryRef = useRef(null);
        // Ref to the speech recognition instance.
        const recognitionRef = useRef(null);

        // Effect hook for auto-scrolling chat history.
        // It triggers whenever the chatHistory state changes.
        useEffect(() => {
            if (chatHistoryRef.current) {
                chatHistoryRef.current.scrollTop = chatHistoryRef.current.scrollHeight;
            }
        }, [chatHistory]);

        // Helper function to convert base64 PCM audio data to a playable WAV blob.
        const base64ToArrayBuffer = (base64) => {
            const binaryString = window.atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        };

        // Helper function to create a WAV file blob from PCM data.
        const pcmToWav = (pcmData, sampleRate) => {
            const buffer = new ArrayBuffer(44 + pcmData.length * 2);
            const view = new DataView(buffer);

            // RIFF chunk descriptor
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + pcmData.length * 2, true);
            writeString(view, 8, 'WAVE');

            // FMT sub-chunk
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true); // Audio format (1 = PCM)
            view.setUint16(22, 1, true); // Number of channels
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * 2, true);
            view.setUint16(32, 2, true); // Block align
            view.setUint16(34, 16, true); // Bits per sample

            // Data sub-chunk
            writeString(view, 36, 'data');
            view.setUint32(40, pcmData.length * 2, true);

            // Write the PCM samples
            for (let i = 0; i < pcmData.length; i++) {
                view.setInt16(44 + i * 2, pcmData[i], true);
            }

            return new Blob([view], { type: 'audio/wav' });
        };

        // Helper function to write a string to a DataView.
        const writeString = (view, offset, string) => {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        };
        
        // Generic API request function with exponential backoff for retries.
        const makeApiRequestWithRetry = async (url, options, retries = 3, delay = 1000) => {
            try {
                const response = await fetch(url, options);
                if (response.ok) {
                    return await response.json();
                }
                if (retries > 0) {
                    await new Promise(res => setTimeout(res, delay));
                    return makeApiRequestWithRetry(url, options, retries - 1, delay * 2);
                }
                throw new Error(`API call failed after multiple retries with status: ${response.status}`);
            } catch (error) {
                console.error('Fetch error:', error);
                if (retries > 0) {
                    await new Promise(res => setTimeout(res, delay));
                    return makeApiRequestWithRetry(url, options, retries - 1, delay * 2);
                }
                throw error;
            }
        };

        // Function to handle Text-to-Speech using the Gemini API.
        const handleSpeak = async (text) => {
            setIsSpeaking(true);
            setIsGeneratingAudio(true);
            try {
                const payload = {
                    contents: [{ parts: [{ text: `Say cheerfully: ${text}` }] }],
                    generationConfig: {
                        responseModalities: ["AUDIO"],
                        speechConfig: {
                            voiceConfig: { prebuiltVoiceConfig: { voiceName: voiceName } }
                        }
                    },
                    model: "gemini-2.5-flash-preview-tts"
                };
                const apiKey = "";
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${apiKey}`;

                const result = await makeApiRequestWithRetry(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const part = result?.candidates?.[0]?.content?.parts?.[0];
                const audioData = part?.inlineData?.data;
                const mimeType = part?.inlineData?.mimeType;

                if (audioData && mimeType && mimeType.startsWith("audio/")) {
                    const matchResult = mimeType.match(/rate=(\d+)/);
                    if (matchResult && matchResult[1]) {
                        const sampleRate = parseInt(matchResult[1], 10);
                        const pcmData = base64ToArrayBuffer(audioData);
                        const pcm16 = new Int16Array(pcmData);
                        const wavBlob = pcmToWav(pcm16, sampleRate);
                        const audioUrl = URL.createObjectURL(wavBlob);

                        const audio = new Audio(audioUrl);
                        audio.onended = () => {
                            setIsSpeaking(false);
                            setIsGeneratingAudio(false);
                            // The answer now persists, so no need to clear it here.
                        };
                        audio.play();
                    } else {
                        throw new Error('Could not parse sample rate from audio mimeType.');
                    }
                } else {
                    throw new Error('Invalid audio data from API');
                }
            } catch (error) {
                console.error('Error with Gemini TTS API:', error);
                setIsSpeaking(false);
                setIsGeneratingAudio(false);
                setCurrentAnswer("I'm sorry, I'm having trouble speaking right now.");
            }
        };
        
        // Toggle between the "Kore" (female) and "Puck" (male) voices.
        const toggleVoice = () => {
            setVoiceName(prev => (prev === "Kore" ? "Puck" : "Kore"));
        };

        // Function to handle the user's speech input and convert it to text.
        const handleListen = () => {
            if (isListening) {
                if (recognitionRef.current) {
                    recognitionRef.current.stop();
                }
                return;
            }

            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            if (!SpeechRecognition) {
                console.error('Your browser does not support the Web Speech API. Please use a modern browser like Chrome.');
                return;
            }
            
            const recognition = new SpeechRecognition();
            recognition.interimResults = true; // Enabled for real-time captions
            recognition.lang = 'en-US';
            recognition.continuous = false; 
            recognitionRef.current = recognition;
            
            let finalTranscript = ''; // Local variable to build the full transcript

            recognition.onstart = () => {
                setIsListening(true);
                setCurrentQuestion('');
                setCurrentAnswer('');
            };
            
            recognition.onresult = (event) => {
                let interimTranscript = '';
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                // Update the display with both final and current interim speech
                setCurrentQuestion(finalTranscript + interimTranscript);
            };

            recognition.onend = () => {
                setIsListening(false);
                const promptToProcess = finalTranscript.trim();
                if (promptToProcess) {
                    // Update currentQuestion one last time with the final, corrected text
                    setCurrentQuestion(promptToProcess);
                    // If a final transcript was captured, process it
                    if (isGeneratingFact) {
                        generateFunFact(promptToProcess);
                    } else {
                        handleUserPrompt(promptToProcess);
                    }
                } else {
                    // If no transcript was captured, clear the display
                    setCurrentQuestion('');
                }
            };

            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                setIsListening(false);
                setIsGeneratingFact(false);
                setCurrentQuestion('');
            };
            
            recognition.start();
        };

        // Function to initiate the fun fact generation process by starting to listen.
        const handleGenerateFunFactButton = () => {
            setIsGeneratingFact(true);
            handleListen();
        };

        // Function to initiate image description by triggering the hidden file input.
        const handleDescribeImageButton = () => {
            fileInputRef.current.click();
        };
        
        // Handles file selection and initiates the image description process.
        const handleFileChange = (event) => {
            const file = event.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = () => {
                    const base64Image = reader.result.split(',')[1];
                    describeImage(base64Image);
                };
                reader.readAsDataURL(file);
            }
        };

        // Function to generate a fun fact using the Gemini API.
        const generateFunFact = async (topic) => {
            setIsGeneratingFact(false);
            setIsLoading(true);
            setIsGeneratingText(true);
            try {
                const prompt = `Generate a very short, interesting, and easy-to-understand fun fact for a school science fair about the topic: "${topic}". The fact should be no more than two sentences.`;
                const payload = { contents: [{ role: 'user', parts: [{ text: prompt }] }] };
                const apiKey = "";
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;

                const result = await makeApiRequestWithRetry(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const text = result?.candidates?.[0]?.content?.parts?.[0]?.text || "I'm sorry, I couldn't generate a fun fact for that topic.";
                
                setIsGeneratingText(false);
                
                // Add the user's prompt and AI's response to the chat history
                setChatHistory(prev => [...prev, { role: 'user', text: `Fun fact about: ${topic}` }, { role: 'ai', text: text }]);
                
                // Display the answer and then speak it.
                setCurrentAnswer(text);
                handleSpeak(text);
                
            } catch (error) {
                console.error('Error fetching from Gemini API:', error);
                const errorMessage = "I'm sorry, I encountered an error while trying to generate a fun fact. Please try again.";
                setChatHistory(prev => [...prev, { role: 'user', text: `Fun fact about: ${topic}` }, { role: 'ai', text: errorMessage }]);
                setCurrentAnswer(errorMessage);
                handleSpeak(errorMessage);
            } finally {
                setIsLoading(false);
            }
        };

        // Function to describe an image using the Gemini Vision API.
        const describeImage = async (base64Image) => {
            setIsLoading(true);
            setIsGeneratingText(true);
            try {
                const prompt = `Describe this image in a simple, concise way, as if explaining it at a school science fair.`;
                const payload = {
                    contents: [
                        {
                            role: "user",
                            parts: [
                                { text: prompt },
                                {
                                    inlineData: {
                                        mimeType: "image/png",
                                        data: base64Image
                                    }
                                }
                            ]
                        }
                    ],
                };
                const apiKey = "";
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;

                const result = await makeApiRequestWithRetry(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const text = result?.candidates?.[0]?.content?.parts?.[0]?.text || "I'm sorry, I couldn't describe that image.";
                
                setIsGeneratingText(false);
                
                // Add the user's prompt and AI's response to the chat history
                setChatHistory(prev => [...prev, { role: 'user', text: 'User uploaded an image.' }, { role: 'ai', text: `Description: ${text}` }]);
                
                // Display the answer and then speak it.
                setCurrentAnswer(text);
                handleSpeak(text);

            } catch (error) {
                console.error('Error fetching from Gemini Vision API:', error);
                const errorMessage = "I'm sorry, I encountered an error while trying to describe the image. Please try again.";
                setChatHistory(prev => [...prev, { role: 'user', text: 'User uploaded an image.' }, { role: 'ai', text: errorMessage }]);
                setCurrentAnswer(errorMessage);
                handleSpeak(errorMessage);
            } finally {
                setIsLoading(false);
            }
        };

        // Function to process the user's question and get a response from the AI.
        const handleUserPrompt = async (prompt) => {
            // Handle specific hardcoded response first
            if (prompt.toLowerCase().includes('who is your creator')) {
                const answer = 'My creators are Renil and Jaan.';
                handleSpeak(answer);
                setChatHistory(prev => [...prev, { role: 'user', text: prompt }, { role: 'ai', text: answer }]);
                return;
            }

            setIsLoading(true);
            setIsGeneratingText(true);
            try {
                const systemPrompt = "You are Right AI, a friendly and knowledgeable AI designed for a school science fair. Your purpose is to answer general knowledge questions. Always maintain a helpful, encouraging, and accurate tone. Keep your answers concise and easy for a school audience to understand. Do not engage in topics outside of general knowledge.";

                const chatHistoryPayload = chatHistory.map(entry => ({
                    role: entry.role === 'user' ? 'user' : 'model',
                    parts: [{ text: entry.text }]
                }));

                // Combine system prompt, existing history, and new user prompt
                const fullPayload = [{ role: 'user', parts: [{ text: systemPrompt }] }, ...chatHistoryPayload, { role: 'user', parts: [{ text: prompt }] }];
                const payload = { contents: fullPayload };
                const apiKey = "";
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?key=${apiKey}`;

                const result = await makeApiRequestWithRetry(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                const text = result?.candidates?.[0]?.content?.parts?.[0]?.text || "I'm sorry, I couldn't find an answer for that.";
                
                setIsGeneratingText(false);

                // Add the user's prompt and AI's response to the chat history
                setChatHistory(prev => [...prev, { role: 'user', text: prompt }, { role: 'ai', text: text }]);

                // Display the answer and then speak it.
                setCurrentAnswer(text);
                handleSpeak(text);
            } catch (error) {
                console.error('Error fetching from Gemini API:', error);
                const errorMessage = "I'm sorry, I encountered an error. Please try again.";
                setChatHistory(prev => [...prev, { role: 'user', text: prompt }, { role: 'ai', text: errorMessage }]);
                setCurrentAnswer(errorMessage);
                handleSpeak(errorMessage);
            } finally {
                setIsLoading(false);
            }
        };

        const handleOpenChatBox = () => {
            setShowChatHistory(true);
        };
        
        const handleCloseChatBox = () => {
            setShowChatHistory(false);
        };

        return (
            <div className="bg-gray-950 text-white min-h-screen flex flex-col items-center justify-center p-4 overflow-hidden relative">
                
                {/* The main AI mouth doodle and text display area */}
                <div className="relative w-72 h-36 md:w-96 md:h-48 lg:w-3/5 lg:h-1/4 flex flex-col items-center justify-center transition-all duration-500 transform scale-100 group">
                    {/* Anime-style mouth SVG with expressions and lip-sync */}
                    <svg viewBox="0 0 100 50" className="w-full">
                        <path 
                            d={isSpeaking ? "M20,30 Q50,15 80,30" : "M20,30 Q50,35 80,30"}
                            fill="none" 
                            stroke="white" 
                            strokeWidth="5" 
                            strokeLinecap="round" 
                            className="transition-all duration-200"
                        />
                    </svg>
                </div>
                
                {/* Processing Symbol */}
                {isLoading && (
                    <div className="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2">
                        <svg className="w-16 h-16 animate-spin text-white" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                            <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                            <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                        </svg>
                    </div>
                )}

                {/* Display area for user question or AI answer */}
                <div className="mt-8 text-center px-4 max-w-xl min-h-[64px]">
                    {isGeneratingText && (
                        <div className="text-xl md:text-2xl font-semibold text-gray-400">
                            Generating response...
                        </div>
                    )}
                    {isGeneratingAudio && (
                        <div className="text-xl md:text-2xl font-semibold text-gray-400">
                            Generating audio...
                        </div>
                    )}
                    {currentQuestion && !isGeneratingText && !isGeneratingAudio && (
                        <div className="text-xl md:text-2xl font-semibold text-gray-200">
                            {currentQuestion}
                        </div>
                    )}
                    {currentAnswer && !isGeneratingText && !isGeneratingAudio && (
                        <div className="text-xl md:text-2xl font-semibold text-gray-200">
                            {currentAnswer}
                        </div>
                    )}
                </div>

                {/* Controls for interaction */}
                <div className="mt-12 flex flex-wrap justify-center gap-4">
                    <button
                        onClick={handleListen}
                        disabled={isLoading}
                        className={`px-6 py-3 rounded-full text-white font-bold transition-all duration-300 transform shadow-lg hover:shadow-xl ${isListening ? 'bg-red-500 hover:bg-red-600 scale-105' : 'bg-blue-500 hover:bg-blue-600'} ${isLoading ? 'opacity-50 cursor-not-allowed' : ''}`}
                    >
                        {isListening ? 'Stop Listening' : 'Start Listening'}
                    </button>
                    <button
                        onClick={handleGenerateFunFactButton}
                        disabled={isLoading}
                        className={`px-6 py-3 rounded-full text-white font-bold transition-colors duration-300 shadow-lg hover:shadow-xl ${isLoading ? 'bg-gray-500 cursor-not-allowed' : 'bg-yellow-500 hover:bg-yellow-600'}`}
                    >
                        {isLoading ? 'Processing...' : '‚ú® Generate Fun Fact'}
                    </button>
                    <button
                        onClick={handleDescribeImageButton}
                        disabled={isLoading}
                        className={`px-6 py-3 rounded-full text-white font-bold transition-colors duration-300 shadow-lg hover:shadow-xl ${isLoading ? 'bg-gray-500 cursor-not-allowed' : 'bg-indigo-500 hover:bg-indigo-600'}`}
                    >
                        {isLoading ? 'Processing...' : 'üñºÔ∏è Describe Image'}
                    </button>
                    <button
                        onClick={handleOpenChatBox}
                        className="px-6 py-3 rounded-full bg-green-500 text-white font-bold hover:bg-green-600 transition-colors duration-300 shadow-lg hover:shadow-xl"
                    >
                        Open Chat Box
                    </button>
                    <button
                        onClick={toggleVoice}
                        disabled={isLoading}
                        className={`px-6 py-3 rounded-full text-white font-bold transition-colors duration-300 shadow-lg hover:shadow-xl ${isLoading ? 'bg-gray-500 cursor-not-allowed' : 'bg-purple-500 hover:bg-purple-600'}`}
                    >
                        {voiceName === "Kore" ? 'Switch to Male Voice' : 'Switch to Female Voice'}
                    </button>
                </div>
                
                {/* Hidden file input for image uploads */}
                <input
                    type="file"
                    ref={fileInputRef}
                    style={{ display: 'none' }}
                    accept="image/*"
                    onChange={handleFileChange}
                />
                
                {/* Chat History Modal/Overlay */}
                {showChatHistory && (
                    <div className="fixed inset-0 bg-gray-950 bg-opacity-90 flex items-center justify-center p-4 z-50">
                        <div className="bg-gray-800 rounded-lg shadow-2xl p-6 w-full max-w-2xl h-full max-h-[80vh] flex flex-col">
                            <div className="flex justify-between items-center border-b border-gray-700 pb-4 mb-4">
                                <h2 className="text-2xl font-bold text-white">Chat History</h2>
                                <button
                                    onClick={handleCloseChatBox}
                                    className="text-gray-400 hover:text-white transition-colors duration-200"
                                >
                                    <svg xmlns="http://www.w3.org/2000/svg" className="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                        <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
                                    </svg>
                                </button>
                            </div>
                            <div ref={chatHistoryRef} className="flex-grow overflow-y-auto space-y-4 flex flex-col-reverse">
                                {chatHistory.length > 0 ? (
                                    chatHistory.slice().reverse().map((message, index) => (
                                        <div 
                                            key={index} 
                                            className={`p-3 rounded-xl shadow-md max-w-[85%] ${message.role === 'user' ? 'bg-blue-600 self-end ml-auto' : 'bg-gray-700 self-start mr-auto'}`}
                                        >
                                            <p className="font-bold">{message.role === 'user' ? 'You' : 'Right AI'}</p>
                                            <p className="whitespace-pre-wrap">{message.text}</p>
                                        </div>
                                    ))
                                ) : (
                                    <p className="text-center text-gray-400 mt-20">No chat history yet.</p>
                                )}
                            </div>
                        </div>
                    </div>
                )}
            </div>
        );
    };
    
    // Self-contained initialization script for the React app.
    // This makes the code more portable and should work on any standard website.
    window.onload = function() {
        let rootElement = document.getElementById('root');
        if (!rootElement) {
            rootElement = document.createElement('div');
            rootElement.id = 'root';
            document.body.appendChild(rootElement);
        }
        
        const root = ReactDOM.createRoot(rootElement);
        root.render(<App />);
    };
    </script>
</body>
</html>
